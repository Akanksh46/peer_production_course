# Dog_Breed_Identification: experience report

## The work I have done:
I used a pre-trained VGG19 model, which is pre-trained on ImageNet for dog breed identification and used Stanford Dogs dataset, to classify dog breeds. The training set for my project contains 12,000 images and 120 categories (100 images for each category) and the test set contains 8580 images and 120 categories[[1]](https://www.kaggle.com/c/dog-breed-identification). In order to fit the number of datasets’ categories, I modified the last fully connected layer. This process of modification involved experimenting with fixing upper layers and training bottom layers. The results of these experiments revealed that it is best to train only the last fully connected layer. I then intuitively concluded that category of the ImageNet includes the category of the dog. We have already known that the process of deep learning will first extract the features of the images and then proceed to classify the images based on the features of the images. We know also that larger-scale ImageNet training systems can achieve more robust features. Because ImageNet already contains a multitude of dog categories, it can effectively extract particular features. Data augmentation, I learned through experimentation, does not improve the overall results because the last layer fully connected layer is just learning the map of the feature space and the label space. But when I modified the last full connected layer without changing others, I got the best result. The extraction of previous features has been very effective with large-scale data sets, and so retraining small-scale datasets to retrieve parameters of features will degrade model performance. The last layer only contains limited parameters which do not need such a large dataset to train. In this project, we used several deep learning tricks, such as early stop, learning rate decay, and data augmentation. In order to prevent overfitting and accelerate convergence, I add batch normalization (BN) function after fully connected layer. So after using BN, we could set a larger learning rate to converge. However, the result is not obvious.

## How it was contributed:
It is not a new project because someone has utilized traditional Machine Learning method to solve it but I create a community called dog_breed_identification manipulating a different deep framework vgg19 and Tensorflow to implement it. Deep learning is far much better on accuracy than traditional machine methods. The baseline method can only get 22% accuracy and the project I did could get 71.6% accuracy.

## Whether and how the work was reviewed:
In this project, there are several students in our community and one of them is very familiar with Deep Learning (He is a leader of the community). When I first time finished my project, I pulled a request in the GitHub and asked him to review it. He said it was the basic version and I could upload to the community for others to learn. And also he put forward an improved issue that using batch normalization to accelerate the speed of convergence. After adding the new function, I found the result cannot be improved obviously. I discussed with the leader and found that BN has little effect on only training the last fully connected layer. I still uploaded the final code to the GitHub and hope more people could discuss with us.

## Compare my experience with my expectations:
What I expected to do in this project is using another deep framework vgg19 which is deeper than vgg16. We compare the performance of traditional Machine Learning methods and Deep Learning. Deep learning is far much better on accuracy because it needs larger data and extracts better features. We only use vgg19 in this project so we could not compare results of it with vgg16’s result.

## Reference
[1] (n.d.). Retrieved March 25, 2018, from https://www.kaggle.com/c/dog-breed-identification
